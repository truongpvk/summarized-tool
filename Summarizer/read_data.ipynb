{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af0fd28d-e4a1-4240-8f6a-0aae897b4f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "!SET SKLEARN_ALLOW_DEPRECATED_SKLEARN_PACKAGE_INSTALL=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36042adc-df68-4915-b98b-9d477a907257",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas numpy scikit-learn nltk langdetect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2048cfa-91ce-4f19-81e0-49aeda261d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b1fc21a9-911a-45e0-8ed1-81ee70005a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = \"wikilingua_multilingual.train.pk\"\n",
    "val = \"wikilingua_multilingual.val.pk\"\n",
    "test = \"wikilingua_multilingual.test.pk\"\n",
    "\n",
    "data_folder = os.path.join(\"Dataset\")\n",
    "\n",
    "train_path = os.path.join(data_folder, train)\n",
    "val_path = os.path.join(data_folder, val)\n",
    "test_path = os.path.join(data_folder, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b090a781-a074-4b41-a5f9-853a9e83fba2",
   "metadata": {},
   "source": [
    "# Read & change language code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "af04330d-12e8-4686-9482-40776afef5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "lang_map = {\n",
    "    'en': 'English',\n",
    "    'es': 'Spanish',\n",
    "    'pt': 'Portuguese',\n",
    "    'fr': 'French',\n",
    "    'de': 'German',\n",
    "    'ru': 'Russian',\n",
    "    'it': 'Italian',\n",
    "    'id': 'Indonesian',\n",
    "    'nl': 'Dutch',\n",
    "    'ar': 'Arabic',\n",
    "    'vi': 'Vietnamese',\n",
    "    'zh': 'Chinese',\n",
    "    'th': 'Thai',\n",
    "    'ja': 'Japanese',\n",
    "    'ko': 'Korean',\n",
    "    'hi': 'Hindi',\n",
    "    'cs': 'Czech',\n",
    "    'tr': 'Turkish',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "24acd5b1-fbec-4f82-8cb2-89737f95ef8b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def read_data(path):\n",
    "    with open(path, \"rb\") as f:\n",
    "        data = pickle.load(f)\n",
    "    df = pd.DataFrame(data)\n",
    "    df.drop(['target', 'id'], axis=1, inplace=True)\n",
    "    df['source'] = df['source'].map(lang_map)\n",
    "    df = df[~df['source'].isin(['Thai'])]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "97412d87-57b3-4a2a-bd7f-daadeec3c0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dts = read_data(train_path)\n",
    "val_dts = read_data(val_path)\n",
    "test_dts = read_data(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3c32e0c0-d449-4cd1-9f6c-efd98952e853",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "source\n",
       "English       95517\n",
       "Spanish       76295\n",
       "Portuguese    54673\n",
       "French        43423\n",
       "German        39505\n",
       "Russian       35313\n",
       "Italian       34085\n",
       "Indonesian    32228\n",
       "Dutch         21345\n",
       "Arabic        19992\n",
       "Vietnamese    13262\n",
       "Chinese       12523\n",
       "Japanese       8657\n",
       "Korean         8370\n",
       "Hindi          6724\n",
       "Czech          4872\n",
       "Turkish        3052\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dts['source'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff7e2d41-10e8-4ac0-bee7-729678e70e04",
   "metadata": {},
   "source": [
    "# Save dts for abstractive model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "259240e4-454e-454d-aa76-9701a67d2631",
   "metadata": {},
   "outputs": [],
   "source": [
    "abtractive_dts = {\n",
    "    'train': train_dts,\n",
    "    'validation': val_dts,\n",
    "    'test': test_dts\n",
    "}\n",
    "\n",
    "with open(os.path.join(data_folder, \"abstractive\", \"abstractive.pk\"), \"wb\") as file:\n",
    "    pickle.dump(abtractive_dts, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36134668-a89e-4421-be6a-78cea353fc0d",
   "metadata": {},
   "source": [
    "# Save dts for extractive model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2211cf0d-6f5a-4a0e-83af-805c3b6a20c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\GIGABYTE\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langdetect import detect\n",
    "import nltk\n",
    "# from bltk.langtools import Tokenizer\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "098da27c-9dee-432e-809f-d3933a1172ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_sentence(text):\n",
    "    try:\n",
    "        lang = detect(text)\n",
    "    except:\n",
    "        return [text]\n",
    "\n",
    "    if lang in ['en', 'es', 'fr', 'de', 'it', 'pt', 'nl']:  # English, Spanish, French, German, etc.\n",
    "        return nltk.sent_tokenize(text, language=lang)\n",
    "\n",
    "    elif lang == 'vi':\n",
    "        return [s.strip() for s in text.replace(\"!\", \".\").replace(\"?\", \".\").split(\".\") if s.strip()]\n",
    "\n",
    "    elif lang == 'zh': # Chinese\n",
    "        import re\n",
    "        return re.split(r'(。|！|\\!|\\.|？|\\?)', text)\n",
    "\n",
    "    elif lang == 'ar': # Arabic\n",
    "        return [s.strip() for s in text.replace(\"؟\", \".\").replace(\"!\", \".\").split(\".\") if s.strip()]\n",
    "\n",
    "    else: # Defautl\n",
    "        return [s.strip() for s in text.replace(\"!\", \".\").replace(\"?\", \".\").split(\".\") if s.strip()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "aafc90f7-d7e7-4a6c-a35f-475198123e00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Masaj, kan akışını hızlandırabilir ve dudaklarını aydınlatabilir',\n",
       " 'Her gece yatmadan önce dudaklarını derinlemesine nemlendirmek için dudaklarına masaj yap',\n",
       " 'Alternatif olarak, dudaklarını nemlendirmek ve daha dolgun ve pembe görünmelerini sağlamak için buz küpleriyle masaj yap',\n",
       " 'Dudaklarını soymanın çeşitli yolları vardır',\n",
       " 'Cilt soyucu ürün kullanmayı tercih edebilirsin ya da dudaklarını bir diş fırçasıyla ovalayabilirsin']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = train_dts.iloc[4, 0]\n",
    "sentences = split_sentence(text)\n",
    "sentences[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f445299b-c0be-4943-9970-fa0524fa46ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python(venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
